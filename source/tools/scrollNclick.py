#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr  1 10:43:42 2019

@author: chenjianyao

Modified by @ZhifeiJin in August 2020
利用浏览器模拟器来达到滚动、点击、跳转页面的需求
"""
import time
import xlrd
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
import os
# import excelSave as save
import tools.login
import requests
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import csv
import re
import pandas as pd
import pickle

username = 'raymondsun97@yahoo.com'  # 微博账号
password = '68458173'  # 微博密码


def init(url="http://weibo.com/login.php"):
    driver = webdriver.Chrome(
        executable_path="/Users/jinzhifei/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver")
    driver.maximize_window()
    driver.get(url)

    login = WebDriverWait(driver, 5).until(
        EC.visibility_of_element_located((By.CLASS_NAME, "login_box")))
    elem = login.find_element_by_xpath("//input[@id='loginname']")
    elem.send_keys(username)
    elem = login.find_element_by_xpath("//input[@name='password']")
    elem.send_keys(password)
    elem = login.find_element_by_xpath('//a[text()="登录"]')
    elem.sendKeys(Keys.RETURN)
    print("结束登录 开始等待")
    time.sleep(60)
    print("结束等待 开始保存cookie")
    # storing the cookies generated by the browser
    cookies = driver.get_cookies()

    # another way to save cookie
    with open('./wb_cookie', 'wb') as f:
        pickle.dump(cookies, f)
    # making a persistent connection using the requests library
    params = {'os_username': username, 'os_password': password}
    s = requests.Session()

    # passing the cookies generated from the browser to the session
    c = [s.cookies.set(c['name'], c['value']) for c in cookies]

    resp = s.post(url, params)  # I get a 200 status_code

    # passing the cookie of the response to the browser
    # driver.delete_all_cookies()
    dict_resp_cookies = resp.cookies.get_dict()
    response_cookies_browser = [{'name': name, 'value': value}
                                for name, value in dict_resp_cookies.items()]
    c = [driver.add_cookie(c) for c in response_cookies_browser]
    driver.implicitly_wait(10)     # 等待时间
    return driver
    # the browser now contains the cookies generated from the authentication

    #options = webdriver.ChromeOptions()
    # options.add_argument('--headless')    # 不打开浏览器
    # options.add_argument('--disable-gpu')    # 禁用GPU硬件加速
    # options.add_argument(
    # 'user-agent="Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1"')  # 添加访问头
    # options.add_argument('proxy-server="60.13.42.109:9999"')    # 添加代理
    # driver = webdriver.Chrome(options=options)   # 使用驱动配置


def getAllSupertopics():
    driver = init("https://huati.weibo.cn/discovery/super?suda")
    info = []
    for category in range(49):
        if category in [4, 6, 8, 13]:
            continue
        for j in range(20):
            # 第一步：点击第[category]个榜单
            class_list = driver.find_element_by_class_name("superListBox")
            superbox = class_list.find_element_by_id("cate_ul")
            # 第一个榜单是“推荐”，在此不计入。html的element从1开始数
            topic_xpath = "li[position()=" + str(category+2) + "]"
            topic = superbox.find_element_by_xpath(topic_xpath)
            ActionChains(driver).click(topic).perform()
            # 第二步：获取榜单[j]个话题的信息
            # 等待加载
            superarea = WebDriverWait(driver, 10).until(
                EC.visibility_of_element_located((By.CLASS_NAME, "super_area")))
            step1 = "//*[@class='card-list'][position()={}]".format(j+1)
            element = driver.find_element_by_xpath(step1)
            supertopic = element.find_element_by_class_name("super_name")
            super_name = supertopic.text
            disc = element.find_element_by_class_name("txt-s").text
            influence = parseNumbers(disc.split()[0])
            follower = parseNumbers(disc.split()[1])
            id = category*20 + j
            # 第2.5步：点击超话，获取话题url，返回榜单页面
            supertopic.click()
            redirect = driver.current_url
            topic_url = redirect.split("3D")[1].split("%")[0]
            result = [super_name, category, id, topic_url, influence, follower]
            print(result)
            info.append(result)
            driver.back()
    print("ready to close")
    driver.close()
    driver.quit()
    return info


"""
用来控制页面滚动
"""


def parseNumbers(source):
    digits = re.search("[0-9]+", source).group(0)
    unit = re.search("\D", source).group(0)
    if unit == "万":
        n = str(int(digits) * 10000)
    else:
        n = digits
    return n


def saveCSV(l):
    with open('SuperTopics.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['SuperTopic', 'Category', 'Id',
                         'Url', 'Influence', 'Follower'])
        for i in l:
            writer.writerow(i)


def repostSpider():
    driver = init()
    data = pd.read_csv("SuperTopics.csv")
    references = data["Url"]
    # print(references)
    supertopic = 0
    weibo = []
    for row in references:
        # print(count)
        url = "https://weibo.com/p/{}/super_index".format(row)
        driver.delete_all_cookies()
        with open('./wb_cookie', 'rb') as f:
            cookies = pickle.load(f)
        for cookie in cookies:
            driver.add_cookie(cookie)
            print(cookie)
        driver.get(url)
        try:
            superarea = WebDriverWait(driver, 2).until(
                EC.visibility_of_element_located((By.CLASS_NAME, "WB_feed")))  # 确认登录超话页面
        except:
            driver.get(url)
        time.sleep(3)
        print("加载成功 开始滚动")
        for page in range(5):
            # 滚动到页面底部
            print("第%d页，共5页" % page)
            for i in range(5):
                driver.execute_script(
                    "window.scrollBy(0,document.body.scrollHeight)", "")
                time.sleep(2)
            print("滚动结束")
            # 确认页面位置
            next = WebDriverWait(driver, 5).until(
                EC.visibility_of_element_located((By.CLASS_NAME, "next")))
            # print(next.get_attribute("outerHTML"))
            # 找到当前页面所有微博
            posts = driver.find_elements_by_class_name("WB_feed_like")
            print("当前有%d条微博" % len(posts))
            for post in posts:
                uid = post.get_attribute("tbinfo").split("=")[1]
                mid = post.get_attribute("mid")
                weibo.append([supertopic, uid, mid])
            next.click()
            try:
                print("尝试输入账号密码登录")
                login = WebDriverWait(driver, 5).until(
                    EC.visibility_of_element_located((By.CLASS_NAME, "layer_login_register_v2")))
                elem = login.find_element_by_xpath("//input[@name='username']")
                elem.send_keys(username)
                elem = login.find_element_by_xpath("//input[@name='password']")
                elem.send_keys(password)
                elem = login.find_element_by_xpath("//a[@class='W_btn_a']")
                elem.click()
            except:
                print("似乎不需要账号密码")
                pass
            print("本页结束 开始等待")
            time.sleep(15)

        supertopic = supertopic + 1
        time.sleep(10)

    # SuperwordRollToTheEnd()


def mid_to_url(midint):
    '''
    http://qinxuye.me/article/mid-and-url-in-sina-weibo/ 
    >>> mid_to_url(3501756485200075)
    'z0JH2lOMb'
    >>> mid_to_url(3501703397689247)
    'z0Ijpwgk7'
    >>> mid_to_url(3501701648871479)
    'z0IgABdSn'
    >>> mid_to_url(3500330408906190)
    'z08AUBmUe'
    >>> mid_to_url(3500247231472384)
    'z06qL6b28'
    >>> mid_to_url(3491700092079471)
    'yCtxn8IXR'
    >>> mid_to_url(3486913690606804)
    'yAt1n2xRa'
    '''
    midint = str(midint)[::-1]
    size = len(midint) / 7 if len(midint) % 7 == 0 else len(midint) / 7 + 1
    result = []
    for i in range(size):
        s = midint[i * 7: (i + 1) * 7][::-1]
        s = base62_encode(int(s))
        s_len = len(s)
        if i < size - 1 and len(s) < 4:
            s = '0' * (4 - s_len) + s
        result.append(s)
    result.reverse()
    return ''.join(result)


def base62_encode(num):
    """Encode a number in Base X

    `num`: The number to encode
    `alphabet`: The alphabet to use for encoding
    """
    alphabet = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
    if (num == 0):
        return alphabet[0]
    arr = []
    base = len(alphabet)
    while num:
        rem = num % base
        num = num // base
        arr.append(alphabet[rem])
    arr.reverse()
    return ''.join(arr)


def SuperwordRollToTheEnd(browser):
    print("transfering clicks")
    try:
        browser.execute_script(
            "window.scrollBy(0,document.body.scrollHeight)", "")
    except:
        pass
    return "Transfer successfully \n"


def SuperwordRollToTheEnd(driver):
    print("scrollin to the end")
    before = 0
    after = 0
    n = 0
    timeToSleep = 50
    for i in range(3):
        before = after
        Transfer_Clicks(driver)
        time.sleep(3)
        #elems = driver.find_elements_by_css_selector('div.WB_feed')
        #print("当前包含超话最大数量:%d,n当前的值为:%d,当n为5无法解析出新的超话" % (len(elems), n))
        #after = len(elems)
        if after > before:
            n = 0
        if after == before:
            n = n + 1
        if n == 3:
            print("当前包含最大超话数为：%d" % after)
            break
        if after > timeToSleep:
            print("抓取到%d多条超话，休眠30秒" % timeToSleep)
            timeToSleep = timeToSleep + 50
            time.sleep(30)
# 插入数据


def insert_data(elems, path, name, yuedu, taolun):
    for elem in elems:
        workbook = xlrd.open_workbook(path)  # 打开工作簿
        sheets = workbook.sheet_names()  # 获取工作簿中的所有表格
        worksheet = workbook.sheet_by_name(sheets[0])  # 获取工作簿中所有表格中的的第一个表格
        rows_old = worksheet.nrows  # 获取表格中已存在的数据的行数
        rid = rows_old
        # 用户名
        weibo_username = elem.find_elements_by_css_selector(
            'h3.m-text-cut')[0].text
        weibo_userlevel = "普通用户"
        # 微博等级
        try:
            weibo_userlevel_color_class = elem.find_elements_by_css_selector(
                "i.m-icon")[0].get_attribute("class").replace("m-icon ", "")
            if weibo_userlevel_color_class == "m-icon-yellowv":
                weibo_userlevel = "黄v"
            if weibo_userlevel_color_class == "m-icon-bluev":
                weibo_userlevel = "蓝v"
            if weibo_userlevel_color_class == "m-icon-goldv-static":
                weibo_userlevel = "金v"
            if weibo_userlevel_color_class == "m-icon-club":
                weibo_userlevel = "微博达人"
        except:
            weibo_userlevel = "普通用户"
        # 微博内容
        weibo_content = elem.find_elements_by_css_selector(
            'div.weibo-text')[0].text
        shares = elem.find_elements_by_css_selector(
            'i.m-font.m-font-forward + h4')[0].text
        comments = elem.find_elements_by_css_selector(
            'i.m-font.m-font-comment + h4')[0].text
        likes = elem.find_elements_by_css_selector(
            'i.m-icon.m-icon-like + h4')[0].text
        # 发布时间
        weibo_time = elem.find_elements_by_css_selector('span.time')[0].text
        print("用户名：" + weibo_username + "|"
              "微博等级：" + weibo_userlevel + "|"
              "微博内容：" + weibo_content + "|"
              "转发：" + shares + "|"
              "评论数：" + comments + "|"
              "点赞数：" + likes + "|"
              "发布时间：" + weibo_time + "|"
              "话题名称" + name + "|"
              "话题讨论数" + yuedu + "|"
              "话题阅读数" + taolun)
        value1 = [[rid, weibo_username, weibo_userlevel, weibo_content,
                   shares, comments, likes, weibo_time, keyword, name, yuedu, taolun], ]
        print("当前插入第%d条数据" % rid)
        save.write_excel_xls_append_norepeat(book_name_xls, value1)
# 获取当前页面的数据


def get_current_weibo_data(elems, book_name_xls, name, yuedu, taolun, maxWeibo):
    # 开始爬取数据
    before = 0
    after = 0
    n = 0
    timeToSleep = 100
    while True:
        before = after
        Transfer_Clicks(driver)
        time.sleep(3)
        elems = driver.find_elements_by_css_selector('div.card.m-panel.card9')
        print("当前包含微博最大数量：%d,n当前的值为：%d, n值到5说明已无法解析出新的微博" % (len(elems), n))
        after = len(elems)
        if after > before:
            n = 0
        if after == before:
            n = n + 1
        if n == 5:
            print("当前关键词最大微博数为：%d" % after)
            insert_data(elems, book_name_xls, name, yuedu, taolun)
            break
        if len(elems) > maxWeibo:
            print("当前微博数以达到%d条" % maxWeibo)
            insert_data(elems, book_name_xls, name, yuedu, taolun)
            break
        if after > timeToSleep:
            print("抓取到%d多条，插入当前新抓取数据并休眠30秒" % timeToSleep)
            timeToSleep = timeToSleep + 100
            insert_data(elems, book_name_xls, name, yuedu, taolun)
            time.sleep(30)
# 点击超话按钮，获取超话页面


def get_superWords():
    time.sleep(5)
    elem = driver.find_element_by_xpath(
        "//*[@class='scroll-box nav_item']/ul/li/span[text()='话题']")
    elem.click()
    # 获取所有超话
    SuperwordRollToTheEnd()
    elemsOfSuper = driver.find_elements_by_css_selector(
        'div.card.m-panel.card26')
    return elemsOfSuper


def get_superwordUrl():
    elemsOfSuper = get_superWords()
    superWords_url = []
    for i in range(0, len(elemsOfSuper)):
        print("当前获取第%d个超话链接，共有%d个超话" % (i+1, len(elemsOfSuper)))
        time.sleep(1)
        element = driver.find_elements_by_css_selector(
            'div.card.m-panel.card26')[i]
        # 获取话题名称，话题讨论书，阅读数
        driver.execute_script('arguments[0].click()', element)
        time.sleep(3)
        print(driver.current_url)
        superWords_url.append(driver.current_url)
        driver.back()
    return superWords_url
# 爬虫运行


def spider(username, password, driver, book_name_xls, sheet_name_xls, keyword, maxWeibo):
    # 加载驱动，使用浏览器打开指定网址
    driver.set_window_size(452, 790)
    driver.get(
        "https://passport.weibo.cn/signin/login?entry=mweibo&res=wel&wm=3349&r=https%3A%2F%2Fm.weibo.cn%2F")
    time.sleep(3)
    # 登陆
    elem = driver.find_element_by_xpath("//*[@id='loginName']")
    elem.send_keys(username)
    elem = driver.find_element_by_xpath("//*[@id='loginPassword']")
    elem.send_keys(password)
    elem = driver.find_element_by_xpath("//*[@id='loginAction']")
    elem.send_keys(Keys.ENTER)
    # 判断页面是否加载出
    while 1:  # 循环条件为1必定成立
        result = isPresent()
        print('判断页面1成功 0失败  结果是=%d' % result)
        if result == 1:
            elems = driver.find_elements_by_css_selector(
                'div.line-around.layout-box.mod-pagination > a:nth-child(2) > div > select > option')
            # return elems #如果封装函数，返回页面
            break
        else:
            print('页面还没加载出来呢')
            time.sleep(20)
    time.sleep(5)
    # 搜索关键词
    elem = driver.find_element_by_xpath("//*[@class='m-text-cut']").click()
    time.sleep(5)
    elem = driver.find_element_by_xpath("//*[@type='search']")
    elem.send_keys(keyword)
    elem.send_keys(Keys.ENTER)

    superWords_url = get_superwordUrl()
    print("超话链接获取完毕，休眠60秒")
    time.sleep(60)
    for url in superWords_url:
        driver.get(url)
        time.sleep(3)
        name = driver.find_element_by_xpath(
            "//*[@class='m-box-col m-box-dir npg-desc']/h4[@class='m-text-cut firsth']").text.replace("热搜", "").replace("\n", "")
        yuedu_taolun = driver.find_element_by_xpath(
            "//*[@class='m-box-col m-box-dir npg-desc']/h4[@class='m-text-cut']").text
        yuedu = yuedu_taolun.split(" ")[0]
        taolun = yuedu_taolun.split(" ")[1]
        time.sleep(5)
        get_current_weibo_data(elems, book_name_xls, name,
                               yuedu, taolun, maxWeibo)  # 爬取综合
        time.sleep(3)
        shishi_element = driver.find_element_by_xpath(
            "//*[@class='scroll-box nav_item']/ul/li/span[text()='实时']")
        driver.execute_script('arguments[0].click()', shishi_element)
        get_current_weibo_data(elems, book_name_xls, name,
                               yuedu, taolun, maxWeibo)  # 爬取实时
        time.sleep(5)
        remen_element = driver.find_element_by_xpath(
            "//*[@class='scroll-box nav_item']/ul/li/span[text()='热门']")
        driver.execute_script('arguments[0].click()', remen_element)
        get_current_weibo_data(elems, book_name_xls, name,
                               yuedu, taolun, maxWeibo)  # 爬取热门
